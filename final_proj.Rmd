---
title: "Final Project"
author: "Anish Shenoy, Jae Yoon Kim"
date: "11/25/2020"
output: pdf_document
---
# Effects of Global Warming Induced Flooding on Low Lying Areas

## Introduction
Global Warming is a present and clear danger to all those living on Earth. The question concerning feeling its effects is no longer if but when. We must understand the near-term effects of global warming as the environmental effects worsen to make adequate changes. We were particularly interested as young people who reside on the coasts, concerned for the probable loss of our beaches and our neighborhoods.

## Question
We would like to explore the demographic and economic changes due to challenges that are brought on by increased flooding near coastal areas as the sea levels rise, focusing on the severity with different flooding conditions over time. More specifically, working with the First Street Climate dataset, we would like to ask which and what types of communities were affected the most by errors in FEMA’s calculations of risk levels. We would like to highlight the potential transformations in inequality brought on by human migration catalyzed by flooding, as well as changes in residential property markets as a result.

## Dataset
We used the “First Street Climate Flood Risk Statistics” dataset which provides the first-ever public evaluation of flood risk for every property in the 48 contiguous states. The dataset provides flood risk statistics and scores at the congressional district, county, ad zip code level. Counties and properties are evaluated for flood risk based on factors such as location, climate, infrastructure, and more. We have paired this data with county-level demographic and economic data provided by tidycensus library in order to evaluate which groups are most impacted by the increased risk of flooding that comes with climate change. We also used the Housing and Urban Development Section 8 voucher data to be able to simulate and predict rent prices when demographic and economic environments change as a result of human migration. Finally, we hope to compare the government-provided “FEMA Flood Map” data with the more-recent First Street dataset to see if the federal maps overestimate or underestimate the flood risk posed by climate change.

## Measurement Issues
Both the FEMA Flood Map and the First Street Climate Flood Risk Statistics are only estimates of risk of flooding in the event of a hypothetical catastrophe. We tried to shore up our measurement issues by relying on First Street’s model which found deficiencies and discrepancies with FEMA’s measurements. 
The housing and urban development’s section 8 voucher program does not accurately reflect the rental prices in real time, however they are a good index to differentiate the expensive, higher cost of living parts of town against the cheaper parts of town, as well as a ballpark number that approximately reflects rental prices. This is also not granular since within zipcodes real estate and rental values can change based on location (i.e. easy access to public transportation, clear views or proximity to the ocean, road noise being next to a busy highway or intersection). 

## Missing Data
There was missing rent data for section 8 housing. We dealt with this by removing data points without rent and working with it. Because rent is based solely on local conditions and not too much on what other non neighboring rents are, we would be unable to predict it with just the mean of the dataset. 

## Inferences/Assumptions
Because the dataset gave us the average risk of flooding in a hypothetical scenario, in the case of predicting rent, we took the risk number and related it with the percent of houses lost from devastation. Although the total_property variable does not reflect the total number of rental properties, it is a good indicator for the number of properties in the region. 
We also assumed that all the other variables would stay the same, most notably the population. Although there is always tragic losses of lives in these events, we made this assumption since deaths even in the worst disasters, such as Katrina (1800) or Irma (134), would be a very small percentage difference due to the overall population of Florida being more than 20 million residents. 





```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rstanarm)
library(ggmap)
library(zipcode)
library(bayesplot)
data(zipcode)
```

## Import, Merge, and Clean data

First, import the First Street Foundation (FSF) Flood Risk Summary Statistics by zip
found here: https://registry.opendata.aws/fsf-flood-risk/?fbclid=IwAR2JUyZWXcXiKdXuXMXON-1VmcUm6RpCnUVXMiydCrrDCaXHg41zdkD-iI8
``` {r echo=TRUE, message=FALSE}
#zip_risk <- read_csv("./data/fsf/v1.1/Zip_level_risk_FEMA_FSF_v1.1.csv")
 zip_risk <- read_csv("/cloud/project/Final/Flood-Data-Analysis-master/data/FSF/v1.1/Zip_level_risk_FEMA_FSF_v1.1.csv")
zip_risk
```
<br>
Next, import data about each zip code in the US uing the tidycensus package
``` {r echo=TRUE, message=FALSE}
library(tidycensus)
census_api_key("0a473d1b1e161d8f87da3ad85e59af583f28f1d6")
# Variable codes found here: 
# https://api.census.gov/data/2010/dec/sf1/variables.html
# P013001: MEDIAN AGE
# H006001: TOTAL ALL RACES IN HOUSEHOLD
# H006002: TOTAL HOUSEHOLDER WHITE ALONE
# H006003: TOTAL BLACK/AFRICAN-AMERICAN ALONE
zip_dem_data <- get_decennial(geography = "zcta",
                          variables = c(median_age = "P013001", 
                                        total_pop = "H006001", 
                                        total_white = "H006002", 
                                        total_black = "H006003"),
                          year = 2010,
                          output = "wide")
zip_income_data <- get_acs(geography = "zcta",
                           variables = c(median_income = "B19013_001"),
                           year = 2018,
                           output = "wide")
```
<br>

Finally, merge the the data by zip code
``` {r echo=TRUE}
merged <- inner_join(zip_dem_data, zip_risk, by = c("GEOID" = "zcta5ce")) %>%
  inner_join(zip_income_data, by = "GEOID") %>%
  select(-NAME.y) %>%
  mutate(prop_white = total_white/total_pop,
         prop_black = total_black/total_pop,
         prop_nonwhite = 1 - prop_white) %>%
  inner_join(zipcode, by=c('GEOID'='zip')) %>%
  # Get rid of counties with no houses at risk now or in the future
  # Make sure we don't have NA's in demographic info
  filter(count_fs_risk_2050_100 > 0,
         count_fs_risk_2020_100 > 0,
         prop_nonwhite >= 0,
         median_incomeE > 0,
         median_age > 0,
         total_pop > 0)
merged
```
\newpage

## Preliminary Data Exploration

Graph of FEMA average risk scores per zip code overlayed across entire Continental USA.
```{r echo=TRUE, message=FALSE}
# Download a map of the CONUS
conus_map<- get_map(location=c(-124.848974, 24.396308, -66.885444, 49.384358), 
                    zoom=5, maptype = 'terrain',
                    source='osm',color='color')
ggmap(conus_map) + 
  geom_point(aes(x=longitude, 
                 y=latitude, 
                 show_guide = TRUE, 
                 colour=avg_risk_score_all), 
             data=merged, alpha=.2, na.rm = T)  + 
  scale_color_gradient(low="beige", high="blue") + 
  labs(title='Average Risk score for each zipcode in CONUS')
```
<br>
This map shows where scores from FEMA disagrees with the FSF data
``` {r echo=TRUE}
# Where did FEMA get it wrong
ggmap(conus_map) + 
  geom_point(aes(x=longitude, 
                 y=latitude, 
                 show_guide = TRUE, 
                 colour = abs(pct_fs_fema_difference_2020)), 
             data=merged, alpha=.2, na.rm = T)  + 
  scale_color_gradient(low="beige", high="blue") + 
  labs(title='FEMA vs FSF')
```
\newpage
Three regions jump out: Kentucky/West Virginia, Coast of Virginia, and the Gulf Coast/panhandle. We will look at each in more detail. 

```{r echo=TRUE, message=FALSE}
# Download a terrain map of the WV to understand why it's flooding so much
wv_bbox <- c(-85, 35, -75, 42)
wv_map<-get_map(location=wv_bbox, zoom=6, maptype = 'terrain',
             source='osm',color='color', size = c(800, 800))
wv_merged <- filter(.data=merged, latitude >= wv_bbox[2], latitude <= wv_bbox[4],
                    longitude >= wv_bbox[1], longitude <= wv_bbox[3])
# Overlay the risk scores to understand
ggmap(wv_map)+ geom_point(
        aes(x=longitude, y=latitude, show_guide = TRUE, colour=avg_risk_score_all), 
        data=wv_merged, alpha=.5, na.rm = T)  + 
        scale_color_gradient(low="beige", high="blue") + 
        labs(title='Average Risk score for each zipcode in WV and Virginia Coast')
```
<br>
The majority of risk in West Virginia comes from around Charleston along the river. 

```{r echo=TRUE, message=FALSE}
# Download a terrain map of the gulf coast to understand why it's flooding so much
fl_bbox <- c(-100, 25, -75, 32)
fl_map<-get_map(location=fl_bbox, zoom=6, maptype = 'terrain',
             source='osm',color='color')
fl_merged <- filter(.data=merged, latitude >= fl_bbox[2], latitude <= fl_bbox[4], 
                    longitude >= fl_bbox[1], longitude <= fl_bbox[3])
# Overlay the risk scores to understand
ggmap(fl_map)+ geom_point(
        aes(x=longitude, y=latitude, show_guide = TRUE, colour=avg_risk_score_all), 
        data=fl_merged, alpha=.5, na.rm = T)  + 
        scale_color_gradient(low="beige", high="blue") + 
        labs(title='Average Risk score for each zipcode in the Gulf Coast')
```

<br>
We would like to look at Florida in more depth. Florida is in a unique position where there are large portions of the state where many people live with high risk scores.
```{r echo=TRUE, message=FALSE}
# Filter data for florida only
fl <- filter(.data=merged, state == 'FL')
fl_only_bbox <- c(-90, 24, -78, 31)
fl_only_merged <- filter(.data=merged, latitude >= fl_only_bbox[2], 
                         latitude <= fl_only_bbox[4], 
                         longitude >= fl_only_bbox[1], 
                         longitude <= fl_only_bbox[3])
fl_only_map<- get_map(location=fl_only_bbox, zoom=6, maptype = 'terrain',
                      source='osm',color='color')
# Overlay the risk scores to understand
# Make the absolute value of the percent diff between FSF and FEMA the size. 
# We care about the magnitude of their error, 
# not necessarily the size for a broad overview.
ggmap(fl_only_map) +
  geom_point(aes(x=longitude, y=latitude, 
                 show_guide = TRUE, 
                 colour=avg_risk_score_all, 
                 size=abs(pct_fs_fema_difference_2020)), 
             data=fl_only_merged, alpha=.4, na.rm = T)  + 
  scale_color_gradient2(low="red", mid="white", high="blue") + 
  labs(title='Average Risk score for each zipcode in Florida')
```
\newpage

In September of 2005, Hurricane Katrina, a Category 5 storm, destroyed New Orleans and was at the time the costliest tropical cyclone on record. There was much criticism of the government response, especially when people began to observe mass negligence and mismanagment, and were even more enraged when they believed it was fueled by race or class. One of the more memorable moments was when rapper, producer, fashion designer, and future presidential nominee Kanye West criticized the George Bush administration, calling them out because he thought "George Bush doesn't care about Black people".

<br>
Since we have the percent difference between FEMA projections and projections by First Street Foundations, we can use them to see if we can find a relationship between it and black/minority percentage in that zipcode.
```{r}
LA_m <- filter(merged, state == 'LA')
yint <- mean(LA_m$pct_fs_fema_difference_2020)
merged %>%
  filter(state == 'LA') %>%
  ggplot(mapping = aes(x = (prop_black),
                       y = pct_fs_fema_difference_2020,
                       )) +
  geom_point(alpha = 0.5) + 
  geom_hline(yintercept = yint,color='red')
```
<br>
Fit a regression to this data
```{r}
LA_fit <- stan_glm(data = LA_m, pct_fs_fema_difference_2020 ~ prop_black +
                     median_incomeE + prop_black:median_incomeE, refresh=0)
LA_fit
```
<br>
We find the regression to be pct_difference = -12.4 + 18.6(prop_black). 
However the standard error is incredibly high and its 95% confidence interval includes 0.
\newpage

## Housing Changes

To get a picture of how the real estate market would change as a result of flooding, let's first try to understand the real estate market. Here, we'll be taking a look at both residential rent prices. Residential home prices are coming from the Housing and Urban Development's Small area fair market rents, which are used to give vouchers for people on section 8 housing. These should be relatively accurate in reflecting current fair market rents. Specifically, we'll be taking a look at the 110% payment standard for a 2 Bedroom house in case our HUD numbers are a little low across all zip codes.

<br>

We would like to limit it to the state of florida since the way that rent prices are determined varies wildly from region to region, much less state to state. Out of 1469 zip codes in Florida, we have residential rental data on 1431, so we'll be predicting on those.
<br>
Let's unpack the dataset and merge it with our existing flooding data. We'll first transform our variables. Since our rent price variable are all clustered between \$1000 to \$2000, let's transform it into the zscore by subtracting the mean and dividing by the standard deviation. We can convert the other variables like meidan income, count_property and total population to the log version since the ranges are a lot larger to lessen the effect of outliers. 
```{r echo=TRUE, message = FALSE}
rent_price <- read_csv("/cloud/project/Final/Flood-Data-Analysis-master/data/fy2021-safmrs.csv")
#rent_price <- read_csv("data/fy2021-safmrs.csv")
# rename columns
names(rent_price)[names(rent_price) == 'ZIP\nCode'] <- 'zip'
names(rent_price)[names(rent_price) == 'HUD Metro Fair Market Rent Area Name'] <- 'RegionName'
names(rent_price)[names(rent_price) == 'SAFMR\n2BR -\n110%\nPayment\nStandard'] <- 'BR2_rent'
# Merge things in
rent_price <- filter(rent_price, grepl("FL", RegionName))
merged_rent <- inner_join(rent_price, merged, by = c("zip" = "GEOID"))
merged_rent <- merged_rent %>% 
  filter(!is.na(BR2_rent)) %>%
  transmute(zip, BR2_rent=parse_number(BR2_rent), total_pop, prop_black, 
            count_property, median_incomeE, pct_fs_fema_difference_2020, 
            avg_risk_score_all, state, city, longitude, latitude)
rent_mean <- mean(merged_rent$BR2_rent)
rent_sd <- sd(merged_rent$BR2_rent)
# Z score the rent, log median income and property count and total_population
merged_rent <- mutate(merged_rent, BR2_z = (BR2_rent - rent_mean) / rent_sd, 
                      log_incomeE = log(median_incomeE), 
                      log_property = log(count_property), log_pop=log(total_pop))
merged_rent <- drop_na(merged_rent)
merged_rent
```
<br>
Let's fit an initial regression to predict rent change. 
```{r echo=TRUE}
rent_fit <- stan_glm(data=merged_rent, 
                     BR2_z ~ log_pop+prop_black+log_property+log_incomeE, 
                     refresh=0)
rent_fit
coef(rent_fit)
```
<br>
For our initial fit, it seems that the standard error isn't too high. What's interesting here is that the number of property is inversely correlated with rent price and total population is directly correlated, which is to be expected of the basic concept of supply and demand. Income seems to also be the most impactful variable, which makes sense since there are rich areas where rich people pay high rent together and the poor people pay low rent together. 

We try to compare different fits by using different predictors and interactors
```{r echo=TRUE}
rent_fit_a <- stan_glm(data=merged_rent, BR2_z ~ log_pop+prop_black+
                         log_property+log_incomeE, refresh=0)
rent_fit_a
rent_fit_b <- stan_glm(data=merged_rent, BR2_z ~ log_pop+prop_black+
                         log_property+log_incomeE+
                         log_incomeE:log_property, refresh=0)
rent_fit_b
rent_fit_c <- stan_glm(data=merged_rent, BR2_z ~ log_pop+prop_black+
                         log_property+log_incomeE+
                         log_property:log_pop, refresh=0)
rent_fit_c
```
Looking at the fits of each, we can see that the standard errors are relatively low and the fits are okay. 

```{r echo=TRUE}
# we compare these with loo_cv
a_loo <- loo(rent_fit_a, k_threshold = 0.7)
b_loo <- loo(rent_fit_b, k_threshold = 0.7)
c_loo <- loo(rent_fit_c, k_threshold = 0.7)
loo_compare(a_loo, b_loo, c_loo)
```
The three regression models after going through leave-one-out cross validation we can see their predictive accuracy. We find that b_fit has the highest predictive accuracy when looking at the elpd_diff relative to the standard error when compared with a_fit and c_fit. We can interpret the coefficients of b_fit. There is an average rise of 0.6 in z score (0.6 * standard deviation rise in rent dollars)  for every rise in 1 point of log of population, or an increase by a multiple of e in population. There is an average rise of 0.7 in z score (0.7 * standard deviation rise in rent dollars)  between having 100% black proportion in the population vs 0% in the population. There is an average rise of 2.1 in z score (2.1 * standard deviation rise in rent dollars)  for every rise in 1 point of log of median income, or an increase by a multiple of e in median income. This trend is reversed with the log of total number of properties. There is an average decrease of 0.4 in z score (0.4 * standard deviation rise in rent dollars)  for every rise in 1 point of log of number of properties, or an increase by a multiple of 1/e in total property count. 

This would make sense since income had the largest slope. 
```{r echo=TRUE}
rent_fit <- rent_fit_b
```
<br>

Simulate a disaster where we lose a certain number of properties. We can do this by multiplying a one-sided random normal number with the risk score and taking that to be the percent of houses damaged and unable to rent to. Depending on how much damage we want, we could move the mean or the standard deviation. We take the new count of property excluding the damaged property, apply the appropriate transformation as before and simulate 10 draws each with the posterior prediction. We take the median of the generated numbers to be the predicted rents to be our prediction.
```{r echo=TRUE}
# simulate a disaster where we lose a certain number of properties 
# take a normal random and multiplied against avg_risk_score which 
# ranges from 0 to 10 and that percentage is lost. 
post_rent <- mutate(.data=merged_rent, 
                    pct_left = 1-((abs(rnorm(1, mean=0, sd=(0.1)))) 
                                  * avg_risk_score_all), 
                    log_property = log(1+pct_left * count_property))
post_rent <- drop_na(post_rent)
# take 10 draws for each and take median
n_draws <- 10
preds <- posterior_predict(rent_fit, newdata=post_rent, draws=n_draws)
preds <- apply(preds, 2, median, na.rm=TRUE)
```
We subtract our prediction from the original rent to find out the change.
```{r}
# subtract the original rent prices from predictions to find 
# estimated nominal rent difference 
merged_rent$delta_rent <- preds * rent_sd - merged_rent$BR2_z * rent_sd
merged_rent
```
```{r}
fl_only_bbox <- c(-88, 25, -79.5, 31)
fl_only_map<-get_map(location=fl_only_bbox, zoom=6, maptype = 'terrain',
             source='osm',color='color')
# Overlay the risk scores to understand
ggmap(fl_only_map)+ geom_point(
        aes(x=longitude, y=latitude, show_guide = TRUE, colour=delta_rent), 
        data=merged_rent, alpha=.5, na.rm = T)  + 
        scale_color_gradient2(low="blue", mid="white", high="red") + 
        labs(title='Change in rent in $ for each zipcode in Florida')
```
As displayed here graphically, we can see that rent actually falls in many places, even with supply falling and assuming that the demand is still there. Some areas, such as the north, 

\newpage

## Demographic analysis
<br>
Here, we try to analyze whether there is a relationship between the demographics of a community and their predicted flood risk
<br>
Set aside some of the data as test data to evaluate our models.
``` {r echo=TRUE}
sims <- sample(nrow(merged), 20000)
train <- slice(merged, sims)
test <- slice(merged, -1*sims)
```

Fit a regression on percentage at risk in 2050 (within 100km) of the county using median age, total population, median income, proportion black, proportion non-white, and percentage at risk in 2020 as predictors. We log transform total_pop, count_property, and median_incomeE to account for their skewness.
``` {r echo=TRUE}
# Fit #0
fit0 <- stan_glm(pct_fs_risk_2050_100 ~ prop_nonwhite + log(median_incomeE) + median_age + log(total_pop) + pct_fs_risk_2020_100,
                 data = train,
                 refresh = 0)
summary(fit0, digits=2)
```
As expected, the coefficient on the percentage of properties at risk in 2020 is close to 1 since it is basically an offset.
We find that the coefficient on proportion nonwhite is 1.92 with a 95% confidence interval of (1.6, 2.24). Since this does not include 0, we can say that there is a positive correlation between a county's risk score and the proportion that is black. Specifically, when the other predictors are held constant, every percentage increase in black population leads to a 2.57% increase in 2050 flood risk.
<br>

Check residual plot for fit #0 using test data.
``` {r echo=TRUE}
preds <- predict(fit0, newdata = test)
res <- test$pct_fs_risk_2050_100 - preds
test %>%
  ggplot(mapping = aes(x = preds,
                       y = res)) + 
  geom_point(color = "red",
             alpha = 0.75) + 
  geom_hline(yintercept = 0) +
  ggtitle("Residual Plot of Fit #0")
```
The fit is not very good. This is because a linear regression (though easy to interpret) is probably not the best fit here since the outcome variable is a proportion that is bounded between 0 and 1.
<br>

Instead, let's fit a linear regression that predicts the number of properties that will be at risk in 2050.
The counts will be log transformed since they are skewed.
``` {r echo=TRUE}
# Fit #1
fit1 <- stan_glm(log(count_fs_risk_2050_100) ~ prop_nonwhite + 
                   log(median_incomeE) + median_age + log(total_pop) + 
                   log(count_fs_risk_2020_100),
                 data = train,
                 refresh = 0)
summary(fit1, digits=2)
```
<br>
As expected, the coefficient on the number of properties at risk in 2020 is 1 since it is basically an offset.
The coefficient on prop_nonwhite is again noticeably high with a tight confidence interval. At 0.09, it tells us that when all else is equal, every percentage increase in nonwhite population causes the log of the number of properties at risk in 2050 to go up by 0.09.
<br>
Check the residual plot for fit #1
``` {r echo=TRUE}
preds <- predict(fit1, newdata = test)
res <- log(test$count_fs_risk_2050_100) - preds
test %>%
  ggplot(mapping = aes(x = preds,
                       y = res)) + 
  geom_point(color = "red",
             alpha = 0.75) + 
  geom_hline(yintercept = 0) +
  ggtitle("Residual Plot for Fit#1")
```
<br>
This residual plot is much better as the points seem to be spread more evenly.
<br>

Finally, since the number of houses at risk is a count, we can try to fit a poisson regression.
``` {r echo=TRUE}
# Poisson regression
fit2 <- stan_glm(count_fs_risk_2050_100 ~ prop_nonwhite + log(median_incomeE) + 
                   median_age + log(total_pop),
                 offset = log(count_fs_risk_2020_100),
                 data = train,
                 family = poisson,
                 refresh = 0)
summary(fit2, digits=4)
```
<br>
Again, we see prop_nonwhite having a very large impact with the highest coefficient of all the predictors. In addition, it has a very small standard error.
<br>
Evaluate this fit by comparing the distribution of the model with the true distribution of the test data.
``` {r echo=TRUE}
# Plot the fit
# Generate 1000 draws
yrep_4 <- posterior_predict(fit2, newdata = test, 
                            offset = log(test$count_fs_risk_2020_100), 
                            draws=1000)
n_sims <- nrow(yrep_4)
sims_display <- sample(n_sims, 100)
ppc_dens_overlay(log10(test$count_fs_risk_2050_100 + 1) , log10(yrep_4[sims_display, ] + 1)) + 
  xlab('log10(y + 1)') +
  theme(axis.line.y = element_blank()) +
  ggtitle("Poisson Fit with Pre-treatment variables")
```
<br>
The model fits fairly well. 
<br>
Now, compare the 3 models using RMSE. All predictions will be converted so that they output the number of properties at risk in 2050.
``` {r echo=TRUE}
# Have every model predict counts
fit0_count_preds <- predict(fit0, newdata = test) * test$count_property
fit1_count_preds <- exp(predict(fit1, newdata = test))
fit2_count_preds <- colMeans(yrep_4)
actual_counts <- test$count_fs_risk_2050_100
fit0_RMSE <- sqrt(mean((fit0_count_preds - actual_counts)^2))
fit1_RMSE <- sqrt(mean((fit1_count_preds - actual_counts)^2))
fit2_RMSE <- sqrt(mean((fit2_count_preds - actual_counts)^2))
paste("Fit #0 RMSE: ", fit0_RMSE)
paste("Fit #1 RMSE: ", fit1_RMSE)
paste("Fit #2 RMSE: ", fit2_RMSE)
```
<br>
The Poisson model fits best, though the second linear regression comes very close.
In both of these models, the coefficients signal that the nonwhite proportion of the population strongly correlates with the number of properties that will be at risk.
\newpage

## Next Steps
	We hope to find additional datasets that may yield more granular data in risk factors, such as geographical data that we were unable to extract from GIS systems, or datasets on natural or artificial systems that would either alleviate or restore normalcy after a disaster has struck. These would help us better understand which zipcodes would perform better in disaster scenarios. Another avenue of research that could be taken is applying more complex models to use to decrease the errors found in our current models. 	

## Conclusion
With regards to the rental data, we find that Florida rental prices are directly correlated with median income, proportion of black, total population but inversely correlated with the number of property. 
In a simulated disaster, we find that  there are different changes in rent with a overall shrink in housing supply. We find that the while the miami areas actually experience a decrease in rent, poorer areas such as the panhandle experience an expected increase in rent. Miami’s result is unexpected as the slope on the log of population was at 0.6, but this may be due to the randomness introduced when we ran simulations or this could be due to Miami’s apparent resistance to flooding. 
With the demographic exploration, we found a clear relationship between the nonwhite proportion of a community and its increase in flood risk from 2020 to 2050. Not only was there a positive correlation between the county's nonwhite proportion and its increase in flood risk, but this correlation was stronger than all other predictors (median income, age, etc.).
However, it is difficult to claim any causal relationship as the number of predictors available was limited and we don't know if there are any external predictors that affect the outcome but weren't included in the model.

































































